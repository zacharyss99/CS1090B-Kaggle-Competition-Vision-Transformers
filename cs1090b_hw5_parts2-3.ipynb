{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce620d02",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> Data Science 2: Advanced Topics in Data Science\n",
    "## Homework 5 Parts 2 & 3: Vision Transformers & Kaggle Competition\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Spring 2025**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Natesh Pillai, and Chris Gumb\n",
    "\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80056def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL\n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\n",
    "    \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/\"\n",
    "    \"content/styles/cs109.css\"\n",
    ").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c851bd-bacf-47ba-86cc-012172014a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import time\n",
    "\n",
    "SEED = 109\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2f786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure notebook runtime\n",
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f54147e",
   "metadata": {},
   "source": [
    "<div style = \"background: lightsalmon; border: thin solid black; border-radius: 2px; padding: 5px\">\n",
    "\n",
    "### Instructions\n",
    "- To submit your notebook, follow the instructions given in on the Canvas assignment page.\n",
    "- Plots should be legible and interpretable *without having to refer to the code that generated them*. They should includelabels for the $x$- and $y$-axes as well as a descriptive title and/or legend when appropriate.\n",
    "- When asked to interpret a visualization, do not simply describe it (e.g., \"the curve has a steep slope up\"), but instead explain what you believe the plot *means*.\n",
    "- Autograding tests are mostly to help you debug. The tests are not exhaustive so simply passing all tests may not be sufficient for full credit.\n",
    "- The use of *extremely* inefficient or error-prone code (e.g., copy-pasting nearly identical commands rather than looping) may result in only partial credit.\n",
    "- We have tried to include all the libraries you may need to do the assignment in the imports cell provided below. Please get course staff approval before importing any additional 3rd party libraries.\n",
    "- Enable scrolling output on cells with very long output.\n",
    "- Feel free to add additional code or markdown cells as needed.\n",
    "- Ensure your code runs top to bottom without error and passes all tests by restarting the kernel and running all cells (note that this can take a few minutes).\n",
    "- **You should do a \"Restart Kernel and Run All Cells\" before submitting to ensure (1) your notebook actually runs and (2) all output is visible**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbaa674-dcf3-43c1-b12a-a81f1bc68037",
   "metadata": {},
   "source": [
    "<a id=\"contents\"></a>\n",
    "\n",
    "# Notebook Contents\n",
    "\n",
    "- [**PART 2 [40 pts]: Vison Transformer from Scratch**](#part2)\n",
    "- [**PART 3 [10 pts]: Kaggle Competition**](#part3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c11a992",
   "metadata": {},
   "source": [
    "<a id=\"part2\"></a>\n",
    "<div class=\"alert alert-success\" style=\"color: #333; background-color: #e6ffe6; border-color: #c3e6cb; border-width: 1px; border-radius: 3px; padding: 30px;\">\n",
    "    \n",
    "# **PART 2: Vision Transformer from Scratch (40 points)**\n",
    "\n",
    "Vision Transformers (ViTs) (for more details, see the original paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)) represent a groundbreaking approach in computer vision by adapting the Transformer architecture, originally designed for natural language processing, to image recognition tasks. The key idea is to split an image into a sequence of patches, treat these patches as tokens (much like words in a sentence), and process them with Transformer models. This paradigm shift has enabled ViTs to achieve state-of-the-art performance on various tasks.\n",
    "\n",
    "In this part of the homework, we will implement a Vision Transformer (ViT) from scratch using TensorFlow. When fine-grained control over the operations in your model is needed, standard Keras layers might not be flexible enough. Creating custom layers, on the other hand, allows you to directly manage each operation within the layer. This not only enables experimentation with novel ideas but also allows you to tailor the model's behavior to its specific requirements. We will build our model using custom layers and train it on a classification task.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Creating a custom layer\n",
    "\n",
    "TensorFlow's design specifies that all custom layers should inherit from `tf.keras.layers.Layer` to ensure they integrate seamlessly into its ecosystem (e.g., automatic differentiation, weight management). Inheritance is a fundamental concept in object-oriented programming (OOP) that allows a class (the child or subclass) to inherit attributes and methods from another class (the parent or superclass).\n",
    "\n",
    "\n",
    "There are **two essential methods** you must implement when creating a custom layer:\n",
    "\n",
    "1. **Constructor (`__init__`)**  \n",
    "   - **Purpose:** Sets up the layer.\n",
    "   - **What to Do:**  \n",
    "     - Inherit from `tf.keras.layers.Layer` by calling `super().__init__()` or `super(class_name, self).__init__()`.\n",
    "     - Initialize any trainable parameters or sub-layers.\n",
    "\n",
    "2. **`call`**  \n",
    "   - **Purpose:** Defines the forward pass (i.e., how inputs are transformed into outputs).\n",
    "   - **What to Do:**  \n",
    "     - Implement the layer’s logic. Whatever computation you need (for example, an element-wise operation) should be written in this method.\n",
    "     - This method is automatically invoked during model training and inference.\n",
    "\n",
    "\n",
    "**Example**\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
    "        self.dense3 = tf.keras.layers.Dense(hidden_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        x = x + inputs\n",
    "        return x\n",
    "```\n",
    "\n",
    "**What Happens in __init__:**\n",
    "- We call `super(ResidualBlock, self).__init__()` to inherit from `tf.keras.layers.Layer`.\n",
    "- We initialize three Dense layers `dense1`, `dense2` and `dense3`, using ReLU activation in the first two, and linear in the last one.\n",
    "\n",
    "**What Happens in call:**\n",
    "- The `call()` method passes the input through the three Dense layers sequentially\n",
    "- a skip connection adds the original input to the output of the seuqnece of Dense layers.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f538d005",
   "metadata": {},
   "source": [
    "## The Quick, Draw! Dataset \n",
    "\n",
    "For this problem, we will be working with a modified version of the [The Quick, Draw! Dataset](https://github.com/googlecreativelab/quickdraw-dataset), which is a large database of 50 million drawings across 345 categories, collected from players of the game [Quick, Draw!](https://quickdraw.withgoogle.com/).\n",
    "\n",
    "For this homework, we will simplify the problem by using only 306,000 images across 24 different classes for training (this includes the data you will use for validation) and a test set of 54,000 images.\n",
    "\n",
    "\n",
    "<a id=\"part2data\"></a>\n",
    "\n",
    "## Downloading the Data Files\n",
    "\n",
    "\n",
    "**The required `train.csv` file is available on [the \"Data\" tab of the CS1090B HW5 Kaggle Competition website](https://www.kaggle.com/t/e067286be2c54c078434793659326610). (DO NOT USE DATA FROM ANY OTHER SOURCE!)**\n",
    "\n",
    "\n",
    "- `train.csv` is our training dataset and the `label` column contains our response class. The 784 other columns correspond to the pixel values of the 28x28 dimension image.  `train.csv` has 306,000 samples. This is the only file needed for this part of the homework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33943718-21a4-40d7-a3a8-27abd51a84d0",
   "metadata": {},
   "source": [
    "## The Quick, Draw! Dataset \n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "\n",
    "![](img/quickdrawpreview.jpg)\n",
    "\n",
    "\n",
    "\n",
    "For this problem, we will be working with a modified version of the [The Quick, Draw! Dataset](https://github.com/googlecreativelab/quickdraw-dataset), which is a large database of 50 million drawings across 345 categories, collected from players of the game [Quick, Draw!](https://quickdraw.withgoogle.com/).\n",
    "\n",
    "For this homework, we will simplify the problem by using only 306,000 images across 24 different classes for training (this includes the data you will use for validation) and a test set of 54,000 images.\n",
    "\n",
    "\n",
    "<a id=\"part2data\"></a>\n",
    "\n",
    "## Downloading the Data Files\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "\n",
    "**The required `train.csv` and `test.csv` data files are available on [the \"Data\" tab of the CS1090B HW5 Kaggle Competition website](https://www.kaggle.com/t/e067286be2c54c078434793659326610). (DO NOT USE DATA FROM ANY OTHER SOURCE!)** Also please check the overview section for important notes [https://www.kaggle.com/competitions/cs-1090-b-hw-5-contest-2025/overview](https://www.kaggle.com/competitions/cs-1090-b-hw-5-contest-2025/overview)\n",
    "\n",
    "Here's a brief description of the data files:\n",
    "\n",
    "- `train.csv` is our training dataset and the `label` column contains our response class. The 784 other columns correspond to the pixel values of the 28x28 dimension image.  `train.csv` has 306,000 samples.\n",
    "\n",
    "- `test.csv` file mirrors the structure of the `train.csv` file **without** the class label column. We have added another column `id`. `id` is a unique identifier for each sample in the test set. This ID is crucial for mapping your model's predictions back to the corresponding samples when submitting your results to Kaggle. `test.csv` has 54,000 samples. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5540a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_csv(data_file):\n",
    "    \"\"\"Load and preprocess data from CSV file\"\"\"\n",
    "    df = pd.read_csv(data_file)\n",
    "\n",
    "    classes = sorted(df['label'].unique().tolist())\n",
    "    \n",
    "    label_to_id = {label: idx for idx, label in enumerate(classes)}\n",
    "    \n",
    "    X = df.drop('label', axis=1).values.astype('uint8')\n",
    "    y = df['label'].map(label_to_id).values \n",
    "    \n",
    "    # Normalize pixel values to [0, 1]\n",
    "    X = X.astype('float32') / 255.0\n",
    "    X = X.reshape(-1, 28, 28, 1)\n",
    "    \n",
    "    return df, X, y, classes, label_to_id  \n",
    "\n",
    "# Load and preprocess the data\n",
    "data_file = \"train.csv\"\n",
    "df, X, y, classes, label_to_id = load_data_from_csv(data_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c559eb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_grid(df, classes, rows=5, cols=5):\n",
    "    \"\"\"\n",
    "    Plot a grid of images for each class from the CSV data\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the image data and labels\n",
    "        classes: List of class names\n",
    "        rows: Number of rows in the grid (default 5)\n",
    "        cols: Number of columns in the grid (default 5)\n",
    "    \"\"\"\n",
    "    # Calculate figure size with reduced height per class\n",
    "    fig_height = len(classes) * 1.5  \n",
    "    fig = plt.figure(figsize=(15, fig_height))\n",
    "    \n",
    "    # Adjust the spacing between subplots\n",
    "    plt.subplots_adjust(hspace=0.4, wspace=0.1)\n",
    "    \n",
    "    for idx, class_name in enumerate(classes):\n",
    "        class_samples = df[df['label'] == class_name].iloc[:rows*cols]\n",
    "        \n",
    "        for i in range(rows*cols):\n",
    "            if i < len(class_samples):\n",
    "                # Get image data, convert to float32 and normalize to [0,1]\n",
    "                img_data = class_samples.iloc[i].drop(['label']).values.astype('float32') / 255.0\n",
    "                img_data = img_data.reshape(28, 28)\n",
    "                \n",
    "                # Create subplot\n",
    "                ax = plt.subplot(len(classes), rows*cols, idx*(rows*cols) + i + 1)\n",
    "                plt.imshow(img_data, cmap='gray')\n",
    "                plt.axis('off')\n",
    "                \n",
    "                # Add class label to the first image of each row\n",
    "                if i == 0:\n",
    "                    ax.text(-0.5, 0.5, class_name, rotation=0, \n",
    "                           ha='right', va='center', transform=ax.transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_image_grid(df, df.label.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60abf8dc",
   "metadata": {},
   "source": [
    "<a id=\"part2\"></a>\n",
    "<div class=\"alert alert-success\" style=\"color: #333; background-color: #e6ffe6; border-color: #c3e6cb; border-width: 1px; border-radius: 3px; padding: 30px;\">\n",
    "    \n",
    "#### Q2.1 Single-Head Self Attention\n",
    "\n",
    "We will build each component of ViT separetely and combine everything in the `VisionTransformer` class. But let's start by implementing self-attention using a custom layer. Define a `SingleHeadAttention` class with the following:\n",
    "\n",
    "\n",
    "**In the `__init__` method** fill in the blanks to initialize:\n",
    "   - `W_query`: A dense layer mapping inputs to tensors with dimension `hidden_dim`\n",
    "   - `W_key`: A dense layer mapping inputs to tensors with dimension `hidden_dim`\n",
    "   - `W_value`: A dense layer mapping inputs to tensors with dimension `hidden_dim`\n",
    "   - `hidden_dim`: The dimension of the projected tensors.\n",
    "\n",
    "**In the `call` method you have to** compute the attention output. As you can recall form the \"Attention Is All You Need\" paper. Given an input tensor with shape `(batch_size, seq_length, input_dim)`, a Single-Head Self Attention layer performs the following operations:\n",
    "   1. **Linear Projections**: Project the input into queries, keys, and values using learned weight matrices:\n",
    "      $$Q = \\text{inputs} \\cdot W_{\\text{query}}$$\n",
    "      $$K = \\text{inputs} \\cdot W_{\\text{key}}$$\n",
    "      $$V = \\text{inputs} \\cdot W_{\\text{value}}$$\n",
    "      \n",
    "      Where each projection transforms from `input_dim` to `hidden_dim`.\n",
    "\n",
    "   2. **Scaled Dot-Product Attention**: Compute attention scores:\n",
    "      $$\\text{attention\\_scores} = \\frac{Q \\cdot K^T}{\\sqrt{\\text{hidden\\_dim}}}$$\n",
    "      $$\\text{attention\\_weights} = \\text{softmax}(\\text{attention\\_scores})$$\n",
    "      $$\\text{attention\\_output} = \\text{attention\\_weights} \\cdot V$$\n",
    "\n",
    "\n",
    "**NOTE**: TensorFlow’s Dense layers perform “lazy” weight initialization – meaning that the layer’s weights aren’t created until the first time the layer is called (when the input shape is known). Therefore, you do not need to know `input_dim` when creating the layer, this will be defined by the input shape, when given to the model.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1f8ddd-6b06-4a25-a1a6-f2f78cd354b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_dim):\n",
    "        \"\"\"\n",
    "        Single-head attention layer.\n",
    "        \n",
    "        Args:\n",
    "            hidden_dim: Dimensionality for queries, keys, and values.\n",
    "        \"\"\"\n",
    "\n",
    "        super(SingleHeadAttention, self).__init__()\n",
    "        # your code\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass for single-head attention.\n",
    "        \n",
    "        Args:\n",
    "            inputs: Tensor of shape (batch_size, seq_length, input_dim).\n",
    "            \n",
    "        Returns:\n",
    "            attention_output: Attention output of shape (batch_size, seq_length, hidden_dim).\n",
    "        \"\"\"\n",
    "        # your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2670227e-8c51-437a-838a-59ae37e3059f",
   "metadata": {},
   "source": [
    "#### Test 1: `SingleHeadAttention` implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1dcf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following test to help debug your implementation so far\n",
    "\n",
    "batch_size = 2\n",
    "seq_length = 8\n",
    "input_dim = 64\n",
    "hidden_dim = 32\n",
    "\n",
    "# Create random input\n",
    "inputs = tf.random.normal((batch_size, seq_length, input_dim))\n",
    "\n",
    "attention = SingleHeadAttention(hidden_dim=hidden_dim)\n",
    "\n",
    "# Forward pass: note that attention(inputs) is equivalent to attention.call(inputs)\n",
    "attention_output = attention(inputs)\n",
    "\n",
    "expected_output_shape = (batch_size, seq_length, hidden_dim)\n",
    "\n",
    "assert attention_output.shape == expected_output_shape, \\\n",
    "    f\"Output shape mismatch: got {attention_output.shape}, expected {expected_output_shape}\"\n",
    "\n",
    "print(f\"SingleHeadAttention output shape: {attention_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b77f29",
   "metadata": {},
   "source": [
    "<a id=\"part2\"></a>\n",
    "<div class=\"alert alert-success\" style=\"color: #333; background-color: #e6ffe6; border-color: #c3e6cb; border-width: 1px; border-radius: 3px; padding: 30px;\">\n",
    "\n",
    "#### Q2.2 Multi-Head Attention Layer\n",
    "\n",
    "The `MultiHeadAttentionLayer` implements multi-head self-attention by processing several single-head attention operations in parallel and then combining their outputs. Given an input tensor with shape `(batch_size, seq_length, input_dim)`, follow these instructions:\n",
    "\n",
    "**In the `__init__` method, you need to:**\n",
    "\n",
    "- **Initialize the number of heads and single-head attention layers:**\n",
    "  - Create a list of attention heads. Each head is an instance of your single-head attention layer, configured with a projection output dimensionality of `head_dim`.  \n",
    "  - For each head $i$ (with $i = 1, \\dots, \\text{num\\_heads}$), the head computes  \n",
    "    $$\n",
    "    \\text{attn\\_output}_i = \\text{SingleHeadAttention}_i(\\text{inputs})\n",
    "    $$\n",
    "    where each $\\text{attn\\_output}_i$ has shape `(batch_size, seq_length, head_dim)`.\n",
    "\n",
    "- **Initialize the output projection layer:**\n",
    "  - Define a Dense layer that projects the concatenated outputs back to the original input dimension (`input_dim`).  \n",
    "  - Once you have concatenated the outputs from all heads, you obtain a tensor of shape `(batch_size, seq_length, head_dim x num_heads)` and the output projection layer maps this tensor to shape `(batch_size, seq_length, input_dim)`.\n",
    "\n",
    "**In the `call` method, you need to:**\n",
    "\n",
    "1. **Process each attention head:**\n",
    "   - Iterate over the list of single-head attention layers and pass the input tensor through each one. This produces, for each head $i$, an output $\\text{attn\\_output}_i$ of shape `(batch_size, seq_length, head_dim)`.\n",
    "\n",
    "2. **Concatenate the outputs:**\n",
    "   - Concatenate all the $\\text{attn\\_output}_i$ tensors along the last (feature) axis:\n",
    "     $$\n",
    "     \\text{concatenated} = \\text{concat}\\left(\\left[\\text{attn\\_output}_1,\\, \\text{attn\\_output}_2,\\, \\dots,\\, \\text{attn\\_output}_{\\text{num\\_heads}}\\right]\\right)\n",
    "     $$\n",
    "     This results in a tensor of shape `(batch_size, seq_length, head_dim × num_heads)`.\n",
    "\n",
    "3. **Apply the output projection:**\n",
    "   - Feed the concatenated tensor into the output projection Dense layer:\n",
    "     $$\n",
    "     \\text{multihead\\_output} = \\text{output\\_projection}(\\text{concatenated})\n",
    "     $$\n",
    "     This yields the final output tensor with shape `(batch_size, seq_length, input_dim)`.\n",
    "\n",
    "This process enables the model to attend to different subspaces of the input simultaneously, thereby capturing a richer representation of the data.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b5d11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_dim, head_dim, num_heads):\n",
    "        \"\"\"\n",
    "        Multi-head attention layer.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Dimensionality of the input and final output.\n",
    "            head_dim: Dimensionality for each attention head.\n",
    "            num_heads: Number of attention heads.\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttentionLayer, self).__init__()\n",
    "        # your code here\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass for multi-head attention.\n",
    "        \n",
    "        Args:\n",
    "            inputs: Tensor of shape (batch_size, seq_length, input_dim).\n",
    "            \n",
    "        Returns:\n",
    "            multihead_output: Multi-head attention output of shape (batch_size, seq_length, input_dim).\n",
    "        \"\"\"\n",
    "        # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe53beb8-b6a4-4dd3-baa8-9cbb5cec0337",
   "metadata": {},
   "source": [
    "#### Test 2: `MultiHeadAttentionLayer` implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e9c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following test to help debug your implementation so far\n",
    "\n",
    "batch_size = 2\n",
    "seq_length = 8\n",
    "input_dim = 64\n",
    "head_dim = 16\n",
    "num_heads = 4\n",
    "\n",
    "# Create random input tensor\n",
    "inputs = tf.random.normal((batch_size, seq_length, input_dim))\n",
    "\n",
    "multihead_attention = MultiHeadAttentionLayer(input_dim=input_dim, head_dim=head_dim, num_heads=num_heads)\n",
    "\n",
    "attention_output = multihead_attention(inputs)\n",
    "\n",
    "expected_output_shape = (batch_size, seq_length, input_dim)\n",
    "\n",
    "assert attention_output.shape == expected_output_shape, (\n",
    "    f\"Output shape mismatch: got {attention_output.shape}, expected {expected_output_shape}\"\n",
    ")\n",
    "\n",
    "print(f\"Multi-head attention output shape: {attention_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965cd5c1",
   "metadata": {},
   "source": [
    "<a id=\"part2\"></a>\n",
    "<div class=\"alert alert-success\" style=\"color: #333; background-color: #e6ffe6; border-color: #c3e6cb; border-width: 1px; border-radius: 3px; padding: 30px;\">\n",
    "\n",
    "#### Q2.3 Transformer Encoder Block\n",
    "\n",
    "Given an input tensor `inputs` of shape `(batch_size, seq_length, embed_dim)` the `TransformerEncoderBlock` performs the following operations:\n",
    "\n",
    "**In the `__init__` method, you need to:**\n",
    "\n",
    "- **Initialize the Multi-Head Attention:**\n",
    "  - Create a `MultiHeadAttentionLayer` that processes the inputs to produce `attn_output = MultiHeadAttentionLayer(inputs)`\n",
    "\n",
    "- **Initialize the Feed-Forward Network:**\n",
    "  - Build a Sequential block consisting of:\n",
    "    - `LayerNormalization` with `epsilon = 1e-5`\n",
    "    - A `Dense` layer with `GELU` activation mapping to `feedforward_dim` units\n",
    "    - A `Dropout` layer with `ffn_dropout_rate`.\n",
    "    - A `Dense` layer mapping back to `embed_dim`\n",
    "    - A `Dropout` layer with `ffn_dropout_rate`.\n",
    "  This block, when applied to an input \\(x\\), computes:\n",
    "    $$\n",
    "    \\text{ffn\\_output} = \\text{FFN}(x)\n",
    "    $$\n",
    "    yielding a tensor with shape `(batch_size, seq_length, embed_dim)`.\n",
    "\n",
    "**In the `call` method, you need to:**\n",
    "\n",
    "1. **Compute the Multi-Head Attention:**\n",
    "   - Apply layer norm to `inputs` with `epsilon = 1e-5`:\n",
    "     $$\n",
    "     \\text{normalized\\_inputs} = \\text{LayerNormalization}(\\text{inputs})\n",
    "     $$\n",
    "   - Apply the multi-head attention layer:\n",
    "     $$\n",
    "     \\text{attn\\_output} = \\text{MultiHeadAttentionLayer}(\\text{normalized\\_inputs})\n",
    "     $$\n",
    "   - Apply `Dropout` with rate `attn_dropout_rate`:\n",
    "     $$\n",
    "     \\text{attn\\_output} = \\text{Dropout}(\\text{attn\\_output})\n",
    "     $$\n",
    "   - Add a residual connection:\n",
    "     $$\n",
    "     x = \\text{inputs} + \\text{attn\\_output}\n",
    "     $$\n",
    "\n",
    "2. **Process through the Feed-Forward Network:**\n",
    "   - Compute the feed-forward output:\n",
    "     $$\n",
    "     \\text{ffn\\_output} = \\text{FFN}(x)\n",
    "     $$\n",
    "   - Apply a second residual connection:\n",
    "     $$\n",
    "     \\text{output} = x + \\text{ffn\\_output}\n",
    "     $$\n",
    "\n",
    "The final output tensor `output` of shape `(batch_size, seq_length, embed_dim)` incorporates both self-attention and feed-forward processing with residual connections.\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139c6a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 embed_dim,\n",
    "                 attention_dim,\n",
    "                 feedforward_dim,\n",
    "                 num_heads,\n",
    "                 ffn_dropout_rate=0.1,\n",
    "                 attn_dropout_rate=0.1,\n",
    "                 epsilon=1e-5):\n",
    "        \"\"\"\n",
    "        A single Transformer encoder block with multi-head attention and feed-forward network.\n",
    "        \n",
    "        Args:\n",
    "            embed_dim: Dimensionality of the input embeddings.\n",
    "            attention_dim: Dimensionality used in each attention head.\n",
    "            feedforward_dim: Dimensionality of the intermediate feed-forward layer.\n",
    "            num_heads: Number of attention heads.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        # your code here\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass for a single Transformer encoder block.\n",
    "        \n",
    "        Args:\n",
    "            inputs: Tensor of shape (batch_size, seq_length, embed_dim).\n",
    "        \n",
    "        Returns:\n",
    "            output: Output tensor of shape (batch_size, seq_length, embed_dim).\n",
    "        \"\"\"\n",
    "        # your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bed1b10-21cb-4bcc-a4c1-36cd4f7b5b4e",
   "metadata": {},
   "source": [
    "#### Test 3: `TransformerEncoderBlock` implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea6ce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following test to help debug your implementation so far\n",
    "\n",
    "batch_size = 2\n",
    "seq_length = 8\n",
    "embed_dim = 32\n",
    "attention_dim = 16\n",
    "feedforward_dim = 64\n",
    "num_heads = 4\n",
    "\n",
    "# Create a random input tensor\n",
    "inputs = tf.random.normal((batch_size, seq_length, embed_dim))\n",
    "\n",
    "encoder_block = TransformerEncoderBlock(\n",
    "    embed_dim=embed_dim,\n",
    "    attention_dim=attention_dim,\n",
    "    feedforward_dim=feedforward_dim,\n",
    "    num_heads=num_heads)\n",
    "\n",
    "output = encoder_block(inputs)\n",
    "\n",
    "expected_output_shape = (batch_size, seq_length, embed_dim)\n",
    "\n",
    "assert output.shape == expected_output_shape, (\n",
    "    f\"Output shape mismatch: got {output.shape}, expected {expected_output_shape}\"\n",
    ")\n",
    "\n",
    "print(f\"TransformerEncoderBlock output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bbc12c",
   "metadata": {},
   "source": [
    "<a id=\"part2\"></a>\n",
    "<div class=\"alert alert-success\" style=\"color: #333; background-color: #e6ffe6; border-color: #c3e6cb; border-width: 1px; border-radius: 3px; padding: 30px;\">\n",
    "\n",
    "#### Q2.4 Transformer Encoder\n",
    "\n",
    "Given an input tensor `inputs` of shape `(batch_size, seq_length, embed_dim)`, the `TransformerEncoder` applies a stack of encoder blocks sequentially. \n",
    "\n",
    "**In the `__init__` method, you need to:**\n",
    "\n",
    "- **Initialize the Encoder Blocks:**\n",
    "  - Create a list of `num_blocks` instances of `TransformerEncoderBlock`, each instantiated with parameters `embed_dim`, `attention_dim`, `feedforward_dim`, and `num_heads`.\n",
    "\n",
    "**In the `call` method, you need to:**\n",
    "\n",
    "1. **Initialize the Representation:**\n",
    "   - Set\n",
    "     $$\n",
    "     x_0 = \\text{inputs} \\in \\mathbb{R}^{\\text{batch\\_size} \\times \\text{seq\\_length} \\times \\text{embed\\_dim}}.\n",
    "     $$\n",
    "\n",
    "2. **Apply Each Encoder Block:**\n",
    "   - For each encoder block $i$, with $i = 1, 2, \\dots, \\text{num\\_blocks}$:\n",
    "     $$\n",
    "     x_i = \\text{TransformerEncoderBlock}_i\\big(x_{i-1}\\big).\n",
    "     $$\n",
    "     \n",
    "3. **Return the Final Output:**\n",
    "   - The final output is \n",
    "     $$\n",
    "     \\text{output} = x_{\\text{num\\_blocks}} \\in \\mathbb{R}^{\\text{batch\\_size} \\times \\text{seq\\_length} \\times \\text{embed\\_dim}}.\n",
    "     $$\n",
    "\n",
    "This stacking operation iteratively refines the input representation through multiple encoder blocks.\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2007e6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, attention_dim, feedforward_dim, num_heads, num_blocks):\n",
    "        \"\"\"\n",
    "        A stack of Transformer encoder blocks.\n",
    "        \n",
    "        Args:\n",
    "            embed_dim: Dimensionality of the input embeddings.\n",
    "            attention_dim: Dimensionality used in each attention head.\n",
    "            feedforward_dim: Dimensionality of the intermediate feed-forward layer.\n",
    "            num_heads: Number of attention heads.\n",
    "            num_blocks: Number of encoder blocks.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        # BEGIN SOLUTION\n",
    "        \n",
    "        # END SOLUTION\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass for the Transformer encoder.\n",
    "        \n",
    "        Args:\n",
    "            inputs: Tensor of shape (batch_size, seq_length, embed_dim).\n",
    "        \n",
    "        Returns:\n",
    "            output: The final output of the encoder (batch_size, seq_length, embed_dim).\n",
    "        \"\"\"\n",
    "        # your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e546d023-68cd-42e6-ad1f-094844fe3acc",
   "metadata": {},
   "source": [
    "#### Test 4: `TransformerEncoder` implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8e5acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following test to help debug your implementation so far\n",
    "\n",
    "batch_size = 2\n",
    "seq_length = 8\n",
    "embed_dim = 32\n",
    "attention_dim = 16\n",
    "feedforward_dim = 64\n",
    "num_heads = 4\n",
    "num_blocks = 3\n",
    "\n",
    "# Create a random input tensor of shape\n",
    "inputs = tf.random.normal((batch_size, seq_length, embed_dim))\n",
    "\n",
    "encoder = TransformerEncoder(embed_dim, attention_dim, feedforward_dim, num_heads, num_blocks)\n",
    "\n",
    "output = encoder(inputs)\n",
    "\n",
    "expected_output_shape = (batch_size, seq_length, embed_dim)\n",
    "\n",
    "assert output.shape == expected_output_shape, (\n",
    "    f\"Output shape mismatch: got {output.shape}, expected {expected_output_shape}\"\n",
    ")\n",
    "\n",
    "print(f\"TransformerEncoder output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0db9cb4",
   "metadata": {},
   "source": [
    "<a id=\"part2\"></a>\n",
    "<div class=\"alert alert-success\" style=\"color: #333; background-color: #e6ffe6; border-color: #c3e6cb; border-width: 1px; border-radius: 3px; padding: 30px;\">\n",
    "\n",
    "#### Q2.5 Patch Embedding\n",
    "\n",
    "The `PatchEmbedding` layer converts an input image into a sequence of patch embeddings. Given an image tensor with shape `(batch_size, image_size, image_size, input_channels)`, the layer performs the following steps:\n",
    "\n",
    "**In the `__init__` method, you need to:**\n",
    "\n",
    "- **Store Relevant Attributes:**  \n",
    "  - Save `image_size`, `input_channels`, `patch_size`, and `num_patches` as class attributes.\n",
    "- **Compute Number of Patches:**  \n",
    "  - Calculate `num_patches` as `(image_size // patch_size) ** 2`.\n",
    "- **Initialize the Projection:**  \n",
    "  - Create a Dense layer (`self.projection`) that will map the flattened patch vector to a vector of dimension `embed_dim`.\n",
    "\n",
    "**In the `call` method, you need to:**\n",
    "\n",
    "1. **Extract Patches:**  \n",
    "   - Use `tf.image.extract_patches` to split the input image into non-overlapping patches. The patches are extracted with:\n",
    "     - `sizes = [1, patch_size, patch_size, 1]`\n",
    "     - `strides = [1, patch_size, patch_size, 1]`\n",
    "     - `rates = [1, 1, 1, 1]`\n",
    "     - `padding = \"VALID\"`\n",
    "   - This produces a tensor of shape `(batch_size, image_size/patch_size, image_size/patch_size, patch_area * input_channels)`.\n",
    "\n",
    "2. **Reshape the Patches:**  \n",
    "   - Flatten the spatial dimensions to convert the patches tensor to shape `(batch_size, num_patches, patch_flat_dim)`, where:\n",
    "     - `patch_flat_dim = patch_size * patch_size * input_channels`.\n",
    "   - **Note**: You might find the function `tf.reshape()` useful in this case.\n",
    "\n",
    "3. **Project the Patches:**  \n",
    "   - Pass the flattened patches through the `Dense` projection layer to obtain the final patch embeddings.\n",
    "   - The output has shape `(batch_size, num_patches, embed_dim)`.\n",
    "\n",
    "This layer effectively transforms each patch (a flattened vector of pixel values) into a learned embedding, serving as the input to subsequent Transformer blocks.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0a15c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, image_size, patch_size, input_channels, embed_dim):\n",
    "        \"\"\"\n",
    "        Converts an image into a sequence of patch embeddings.\n",
    "        \n",
    "        Args:\n",
    "            image_size: Size (height/width) of the input image (assumed square).\n",
    "            patch_size: Size of each (square) patch.\n",
    "            input_channels: Number of channels in the input image.\n",
    "            embed_dim: Dimensionality of the patch embeddings.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        # your code here\n",
    "\n",
    "    def call(self, images):\n",
    "        \"\"\"\n",
    "        Forward pass for patch embedding.\n",
    "        \n",
    "        Args:\n",
    "            images: Tensor of shape (batch_size, height, width, input_channels).\n",
    "        \n",
    "        Returns:\n",
    "            A tensor of shape (batch_size, num_patches, embed_dim).\n",
    "        \"\"\"\n",
    "        # your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6b534a-b029-4f15-a163-7ec66fa8c9d1",
   "metadata": {},
   "source": [
    "#### Test 5: `PatchEmbedding` implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5378e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following test to help debug your implementation so far \n",
    "\n",
    "batch_size = 2\n",
    "image_size = 32\n",
    "patch_size = 8\n",
    "input_channels = 3\n",
    "embed_dim = 64\n",
    "\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "# Create a random input tensor representing images\n",
    "images = tf.random.normal((batch_size, image_size, image_size, input_channels))\n",
    "\n",
    "patch_embedding = PatchEmbedding(\n",
    "    image_size=image_size,\n",
    "    patch_size=patch_size,\n",
    "    input_channels=input_channels,\n",
    "    embed_dim=embed_dim\n",
    ")\n",
    "\n",
    "output = patch_embedding(images)\n",
    "\n",
    "expected_output_shape = (batch_size, num_patches, embed_dim)\n",
    "\n",
    "assert output.shape == expected_output_shape, (\n",
    "    f\"Output shape mismatch: got {output.shape}, expected {expected_output_shape}\"\n",
    ")\n",
    "\n",
    "print(f\"PatchEmbedding output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a992c7e8",
   "metadata": {},
   "source": [
    "<a id=\"part2\"></a>\n",
    "<div class=\"alert alert-success\" style=\"color: #333; background-color: #e6ffe6; border-color: #c3e6cb; border-width: 1px; border-radius: 3px; padding: 30px;\">\n",
    "\n",
    "#### Q2.6 Vision Transformer\n",
    "\n",
    "The `VisionTransformer` model will put all these pieces together. To help a little bit, we filled the constructor for you. Given an input tensor with shape  `(batch_size, height, width, input_channels)` where `height = width = image_size`, the model operates as follows:\n",
    "\n",
    "**In the `__init__` method:**\n",
    "\n",
    "- **Determine Input Parameters:**\n",
    "  - Extract `image_size` and `input_channels` from `input_size`.\n",
    "- **Create the Patch Embedding Layer:**\n",
    "  - The `PatchEmbedding` layer splits the image into non-overlapping patches and projects each patch to an embedding vector of dimension `embed_dim`.\n",
    "  - The total number of patches is computed as `num_patches = (image_size // patch_size)^2`\n",
    "- **Define Positional Embedding:**\n",
    "  - Create a positional embedding using `tf.keras.layers.Embedding` with an input dimension equal to `num_patches` and an output dimension `embed_dim`.\n",
    "- **Define a Learnable Class Token:**\n",
    "  - Initialize a class token of shape `(1, 1, embed_dim)`, which is later broadcasted to every batch.\n",
    "- **Instantiate the Transformer Encoder:**\n",
    "  - Create a `TransformerEncoder` that contains `num_blocks` encoder blocks.  \n",
    "    Each block applies multi-head attention and a feed-forward network (with their respective dropout, residual connections, and layer normalization) to process the input.\n",
    "- **Define the Classification Head:**\n",
    "  - Build a sequential model comprising a `LayerNormalization` (with `epsilon=1e-5`) followed by a Dense layer mapping to `num_classes`.\n",
    "\n",
    "**In the `call` method, you need to:**\n",
    "\n",
    "1. **Compute Patch Embeddings:**\n",
    "   - Pass `images` through `self.patch_embedding` to obtain `patch_embeddings` of shape `(batch_size, num_patches, embed_dim)`\n",
    "2. **Add Positional Embeddings:**\n",
    "   - Generate a sequence of positions (of length `num_patches`) and map them through `self.position_embedding` to obtain positional embeddings:  \n",
    "     `positional_embeddings` of shape `(1, num_patches, embed_dim)`\n",
    "   - Add these positional embeddings to `patch_embeddings` elementwise.\n",
    "3. **Prepend the Class Token:**\n",
    "   - Broadcast the learnable class token to match the batch size, resulting in a tensor of shape `(batch_size, 1, embed_dim)`.\n",
    "   - Concatenate this class token with the positional-enhanced patch embeddings along the sequence dimension, yielding:  \n",
    "     `transformer_input` with shape `(batch_size, num_patches + 1, embed_dim)`\n",
    "4. **Apply the Transformer Encoder:**\n",
    "   - Pass `transformer_input` through `self.transformer_encoder` which outputs `encoder_output` with shape `(batch_size, num_patches + 1, embed_dim)`\n",
    "5. **Classification:**\n",
    "   - The classification head uses the embedding corresponding to the class token (index 0) to compute logits `logits`, with shape`(batch_size, num_classes)`\n",
    "\n",
    "The model thus returns the `logits` that represent the class scores for each input image.\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c95feca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(tf.keras.Model):\n",
    "    # We'll give you the init ;)\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        patch_size,\n",
    "        num_heads,\n",
    "        num_blocks,\n",
    "        embed_dim,\n",
    "        attention_dim,\n",
    "        feedforward_dim,\n",
    "        input_size=(28,28,1)\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Vision Transformer model implementation.\n",
    "        \n",
    "        Args:\n",
    "            num_classes: Number of output classes.\n",
    "            patch_size: Size of each patch.\n",
    "            num_heads: Number of attention heads.\n",
    "            num_blocks: Number of Transformer encoder blocks.\n",
    "            embed_dim: Dimensionality of the patch/position embeddings.\n",
    "            attention_dim: Dimensionality used in each attention head.\n",
    "            feedforward_dim: Dimensionality of the intermediate feed-forward layer.\n",
    "            input_size: Dimensionality of the input\n",
    "        \"\"\"\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        image_size = input_size[0]\n",
    "        input_channels = input_size[-1]\n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            image_size, patch_size, input_channels, embed_dim\n",
    "        )\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Positional embedding for each patch\n",
    "        self.position_embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=self.num_patches,\n",
    "            output_dim=embed_dim\n",
    "        )\n",
    "\n",
    "        # Learnable class token used for classification\n",
    "        self.cls = self.add_weight(\n",
    "            \"cls\",\n",
    "            shape=(1, 1, embed_dim),\n",
    "            initializer=tf.random_normal_initializer()\n",
    "        )\n",
    "\n",
    "        self.transformer_encoder = TransformerEncoder(\n",
    "            embed_dim=embed_dim,\n",
    "            attention_dim=attention_dim,\n",
    "            feedforward_dim=feedforward_dim,\n",
    "            num_heads=num_heads,\n",
    "            num_blocks=num_blocks\n",
    "        )\n",
    "\n",
    "        # Classification head: LayerNorm -> Dense\n",
    "        self.classification_head = tf.keras.Sequential([\n",
    "            tf.keras.layers.LayerNormalization(epsilon=1e-5),\n",
    "            tf.keras.layers.Dense(num_classes)\n",
    "        ])\n",
    "\n",
    "    def call(self, images):\n",
    "        \"\"\"\n",
    "        Forward pass through the Vision Transformer model.\n",
    "        \n",
    "        Args:\n",
    "            images: Tensor of shape (batch_size, height, width, input_channels).\n",
    "        \n",
    "        Returns:\n",
    "            logits: Logits of shape (batch_size, num_classes).\n",
    "        \"\"\"\n",
    "        # your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3f8002",
   "metadata": {},
   "source": [
    "<a id=\"part2\"></a>\n",
    "<div class=\"alert alert-success\" style=\"color: #333; background-color: #e6ffe6; border-color: #c3e6cb; border-width: 1px; border-radius: 3px; padding: 30px;\">\n",
    "\n",
    "#### 2.7 Model training and evaluation\n",
    "\n",
    "##### Q2.7.1 Create data splits\n",
    "   - Create train and validation splits from your dataset `(X, y)` using `train_test_split` with `test_size=0.1` and `random_state=109`.\n",
    "   - Create the training and validation datasets using `tf.data.Dataset.from_tensor_slices()`, then batch them using a batch size of 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcbb879-1df4-4bd2-8fdb-3d3453708f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3f7a60",
   "metadata": {},
   "source": [
    "<a id=\"part2\"></a>\n",
    "<div class=\"alert alert-success\" style=\"color: #333; background-color: #e6ffe6; border-color: #c3e6cb; border-width: 1px; border-radius: 3px; padding: 30px;\">\n",
    "\n",
    "\n",
    "##### Q2.7.2 Model training and evaluation \n",
    "   Create a `VisionTransformer` with the following configuration:\n",
    "   - `num_classes = 24`\n",
    "   - `patch_size = 4`\n",
    "   - `num_heads = 8`\n",
    "   - `num_blocks = 4`\n",
    "   - `embed_dim = 64`\n",
    "   - `attention_dim = 64`\n",
    "   - `feedforward_dim = 128`\n",
    "\n",
    "After instantiating your model, call the `build` method with an input shape of `(None, 28, 28, 1)` and show a summary of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8ce07c-af4b-443f-a6aa-250d28237e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc036a2",
   "metadata": {},
   "source": [
    "<a id=\"part2\"></a>\n",
    "<div class=\"alert alert-success\" style=\"color: #333; background-color: #e6ffe6; border-color: #c3e6cb; border-width: 1px; border-radius: 3px; padding: 30px;\">\n",
    "\n",
    "##### Q2.7.3 Model training\n",
    "\n",
    "Train the model for 20 epochs using `Adam` with learning rate of 0.005, `SparseCategoricalCrossentropy` as loss function, and `accuracy` as metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e264d9ef-9b32-4b7c-b78d-598f4918386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eced84",
   "metadata": {},
   "source": [
    "<a id=\"part2\"></a>\n",
    "<div class=\"alert alert-success\" style=\"color: #333; background-color: #e6ffe6; border-color: #c3e6cb; border-width: 1px; border-radius: 3px; padding: 30px;\">\n",
    "\n",
    "#### Q2.7.4 Model Evaluation\n",
    "\n",
    "Visualize the training progress by plotting both the training and validation loss and accuracy over the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf0bf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ca1b1b-ee09-4f40-95d0-5e918621467b",
   "metadata": {},
   "source": [
    "<a id=\"part2\"></a>\n",
    "<div class=\"alert alert-success\" style=\"color: #333; background-color: #e6ffe6; border-color: #c3e6cb; border-width: 1px; border-radius: 3px; padding: 30px;\">\n",
    "\n",
    "Then, evaluate the model on the training and validation sets and print the corresponding loss and accuracy values. You should get a validation accuracy >= 77 if you did everything right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b967cae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03357ed4-9b5a-4f47-a67b-38539e7fc258",
   "metadata": {},
   "source": [
    "<a id=\"part3\"></a>\n",
    "# PART 3 [10 pts]: The Quick, Draw!  Kaggle competition\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "<a id=\"part3kaggle\"></a>\n",
    "\n",
    "## CS1090B Kaggle Competition\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "**ACCESS AND JOIN THE COMPETITION**:\n",
    "\n",
    "**You need to create an account on Kaggle and [join the competition via this link](https://www.kaggle.com/t/e067286be2c54c078434793659326610). This is a limited participation competition. Please DO NOT share this link.**\n",
    "\n",
    "**For more information on the rules** governing this CS1090B Kaggle competition, please see below and also review. The modeling restrictions DOS and DON'TS are outlined in Kaggle Overview page.\n",
    "\n",
    "**IMPORTANT NOTES ABOUT SCORING**:\n",
    "\n",
    "- `sample_submission.csv` is the format that kaggle will accept. The uploaded `.csv` must contain 2 columns. The first column must be named `id` and needs to contain the test observation index numbers for each prediction, the second must be named `label` and needs to contain your class predictions (i.e. 'cat', 'tiger' etc) for each corresponding test observation index location. \n",
    "- The **public leaderboard** on Kaggle displays your performance on 45% of the test set.\n",
    "\n",
    "\n",
    "- After the competition is complete, the **private leaderboard** will show your performance on the remaining 55% of the test set.\n",
    "\n",
    "\n",
    "**ADDITIONAL COMPETITION RULES:**\n",
    "\n",
    "- Multiple Kaggle submissions are permitted (with a maximum of 5 submissions per team per-day), **just note that you will need to choose, on Kaggle, the ONE single submission to use for final scoring prior to the final HW5 submission deadline**, and **your submitted notebook MUST contain the matching code and model that generated your ONE chosen submission.**\n",
    "\n",
    "\n",
    "- **To repeat this point, the version of your final HW5 notebook submitted on Canvas MUST contain the same code and exact same model used to generate your ONE chosen Kaggle submission.** (TFs may rerun your notebook code to ensure comparable final leaderboard results.)\n",
    "\n",
    "\n",
    "- **Please do not manually label your submissions.** In other words, the labels should only be the outcome of your model.\n",
    "\n",
    "\n",
    "- **No external data are allowed, you MUST USE ONLY the training and test data downloaded via the \"Data\" tab of [the CS1090B competition page linked above](#part2data).**\n",
    "\n",
    "- **Do not** create multiple accounts on Kaggle.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce64c618-aa2c-42ee-b504-1323901f58d6",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-success\" style=\"color: #333; background-color: #e6e6fa; border-color: #d8bfd8; border-width: 1px; border-radius: 3px; padding: 10px;\">\n",
    "\n",
    "<a id=\"q3.1\"></a>\n",
    "\n",
    "**3.1.1**  **Kaggle Competition**\n",
    "\n",
    "Create a model and use it to compete on Kaggle.\n",
    "\n",
    "\n",
    "**IMPORTANT: YOU MUST** ensure that the version of the code and model in your final submitted notebook is the **EXACT SAME** code and model used to generate your Kaggle submission. TFs may run your submitted model to ensure comparable results.\n",
    "\n",
    "**Other Kaggle competition rules and scoring details [are listed here](#part2kaggle).\n",
    "[https://www.kaggle.com/competitions/cs-1090-b-hw-5-contest-2025/overview](https://www.kaggle.com/competitions/cs-1090-b-hw-5-contest-2025/overview)**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78c53a41-5bd0-4c8c-b3b7-38cd042e6729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df34cbd7-19ac-4a81-9b94-88ebbbc14d74",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-success\" style=\"color: #333; background-color: #e6e6fa; border-color: #d8bfd8; border-width: 1px; border-radius: 3px; padding: 10px;\">\n",
    "\n",
    "\n",
    "**3.1.2**  Plot your model's training accuracy and and any relevant metrics as a function of epochs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9fa456c-8f43-49e7-9302-fbf5c612240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab161553-54cb-41e1-a90d-b0ec8b081503",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-success\" style=\"color: #333; background-color: #e6e6fa; border-color: #d8bfd8; border-width: 1px; border-radius: 3px; padding: 10px;\">\n",
    "\n",
    "\n",
    "**3.1.3**  In a few sentences, describe the various approaches you have taken to improve the performance of your model as well as any observations you might have regarding your training and Kaggle results.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1e472e-8c99-4fef-a79e-616f904cf6d0",
   "metadata": {},
   "source": [
    "**APPROACH AND OBSERVATIONS:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6a5dc6-3749-429a-bff9-993e89febeb2",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a613a6-d5ce-4ded-aa85-0f0d2a71d766",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-success\" style=\"color: #333; background-color: #e6e6fa; border-color: #d8bfd8; border-width: 1px; border-radius: 3px; padding: 10px;\">\n",
    "\n",
    "**3.2.1**  Generate your test-set class predictions using your regularized model. Save those predictions to a `.csv` formatted file. Submit that `.csv` file [to the CS1090B Kaggle Competition](https://www.kaggle.com/competitions/cs-1090-b-hw-5-contest-2025/) for leaderboard scoring. \n",
    "\n",
    "**IMPORTANT:** For Kaggle to accept and score your submitted `.csv` file, it MUST contain 2 columns. The first column must be named `\"id\"` and needs to contain the test observation index numbers corresponding to each of your 54,000 predictions, the second column must be named `\"label\"` and needs to contain your class predictions (i.e. `cat` or `dog` etc.) for each corresponding test observation index location. A sample submission file is provided on [Kaggle](https://www.kaggle.com/competitions/cs-1090-b-hw-5-contest-2025/data?select=sample_submission.csv)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1337543-e4ab-41d4-8f9a-7bdf49a47c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254bfc78-693e-444d-b7be-16b842e17b92",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-success\" style=\"color: #333; background-color: #e6e6fa; border-color: #d8bfd8; border-width: 1px; border-radius: 3px; padding: 10px;\">\n",
    "\n",
    "\n",
    "**3.2.2**  **Specify your Kaggle name that you have used on the leaderboard**. We CANNOT give you credit without this.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a69b5a-09e9-46b4-82bf-141b74eb0586",
   "metadata": {},
   "source": [
    "**YOUR KAGGLE LEADERBOARD NAME:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e012d644-3b19-45bc-94dc-21012a1b5428",
   "metadata": {},
   "source": [
    "*Your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a33e192-91fa-4b63-88f0-7134424af787",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\" style=\"color: #333; background-color: #e6e6fa; border-color: #d8bfd8; border-width: 1px; border-radius: 3px; padding: 10px;\">\n",
    "<b>3.3 Wrap-up</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a92c0b-8ef6-4b74-be52-8b82495882b3",
   "metadata": {},
   "source": [
    "* In a few sentences, please describe the aspect(s) of the assignment you found most challenging. This could be conceptual and/or related to coding and implementation.\n",
    "\n",
    "* How many hours did you spend working on this assignment? Store this as an int or float in `hours_spent_on_hw`. If you worked on the project in a group, report the *average* time spent per person."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47584bd5-dba1-4890-80a1-0ed019a28fe3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3206bd9-392c-4ceb-b754-e6ee565b914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_spent_on_hw = ... # np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccda7857",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_end = time.time()\n",
    "print(f\"It took {(time_end - time_start)/60:.2f} minutes for this notebook to run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df1f80b",
   "metadata": {},
   "source": [
    "**This concludes HW5... the final CS 1090B HW! Thank you!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66020f1a-4441-493d-9191-b714f50962be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
